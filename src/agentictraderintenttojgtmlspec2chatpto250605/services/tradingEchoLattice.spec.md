# Trading Echo Lattice (Memory Crystal) Service Specification

## 1. Purpose

The `Trading Echo Lattice` serves as the persistent memory and historical record for the entire "Trader Intent to Execution" lifecycle. It captures data from each stage of the process, from the initial trader narrative through LLM translation, spec parsing, signal processing, script generation, trade execution, and subsequent outcomes and feedback. This "memory crystal" is foundational to the Echo Spiral concept, enabling learning, refinement, analysis, and the continuous improvement of both trading strategies and the automated translation process.

## 2. Inputs (Data to be Logged)

The Lattice is primarily an input-driven system, designed to receive and store data from various other components.

*   **Trader Intent Data**:
    *   `narrative_id` (Unique identifier for this entire flow instance)
    *   `original_trader_narrative` (Text)
    *   `submission_timestamp`
    *   `trader_id` (Optional, if multi-user)
*   **LLM Translation Data**:
    *   `jgtml_spec_json` (The raw JSON spec generated by the LLM)
    *   `llm_model_used` (e.g., 'gemini-2.5-flash-preview-04-17')
    *   `llm_prompt_version_or_hash` (To track changes in prompting)
    *   `llm_translation_timestamp`
    *   `llm_processing_duration` (Optional)
    *   `llm_confidence_score` (If available from the API)
    *   `llm_status` (Success/Failure, error messages)
*   **IntentSpecParser Data**:
    *   `parser_status` (Success/Failure)
    *   `parser_message`
    *   `validated_jgtml_spec_json` (Could be same as LLM output if no changes, or the cleaned/validated version)
    *   `parsing_timestamp`
*   **JGTML Signal Processing Data**:
    *   `processed_signal_package_json` (Output from the signal processing service)
    *   `signal_processing_status` (Success/Failure, error messages)
    *   `signal_processing_timestamp`
*   **EntryScriptGen Data**:
    *   `generated_scripts_info` (Array of {`script_name`, `target_platform`, `script_hash_or_content`})
    *   `script_generation_status`
    *   `script_generation_timestamp`
*   **Trade Execution Data (From trading platform/broker, potentially via an adapter)**:
    *   `trade_id` (Platform-specific trade identifier)
    *   `order_id` (Platform-specific order identifier)
    *   `instrument_traded`
    *   `action_taken` (BUY/SELL)
    *   `entry_price_actual`
    *   `exit_price_actual` (If closed)
    *   `quantity_filled`
    *   `commission_paid`
    *   `slippage_amount`
    *   `trade_entry_timestamp`
    *   `trade_exit_timestamp` (If closed)
    *   `profit_loss_amount`
    *   `trade_status` (e.g., Open, Closed, Rejected, Partially Filled)
    *   `execution_feedback_from_platform` (e.g., error messages)
*   **Market Context Data (Snapshots)**:
    *   Key market indicators or conditions at the time of:
        *   Narrative submission.
        *   Signal generation.
        *   Trade entry/exit.
    *   (e.g., VIX level, news event flags, broader market sentiment score). This requires a separate data collection mechanism.
*   **User Feedback Data (Manually or programmatically entered)**:
    *   `feedback_type` (e.g., "LLM_translation_quality", "signal_validity", "trade_outcome_review")
    *   `rating_score` (e.g., 1-5 stars)
    *   `qualitative_feedback_text`
    *   `suggested_improvements` (e.g., for LLM prompt, for signal logic)
    *   `feedback_timestamp`

## 3. Processing Logic (Internal Operations)

1.  **Data Ingestion & Storage**:
    *   Provide secure and reliable API endpoints or interfaces for other services to submit their data.
    *   Validate incoming data against a defined schema.
    *   Store data in a structured, persistent database (e.g., relational like PostgreSQL, document-oriented like MongoDB, or a graph database like Neo4j, depending on query needs).
2.  **Data Linking & Correlation**:
    *   Crucially, establish and maintain relationships between all pieces of data related to a single `narrative_id` or trading idea. For example, link a trade outcome back to the `JGTMLSpec`, the original narrative, and the user feedback on that trade.
    *   Use foreign keys, embedded documents, or graph relationships as appropriate.
3.  **Data Indexing**:
    *   Index key fields to allow for efficient querying and retrieval of data (e.g., by `narrative_id`, `instrument`, `timestamp`, `trader_id`).
4.  **Data Archival & Retention Policies (Long-term consideration)**:
    *   Implement policies for how long data is kept and how it might be archived.
5.  **API for Data Retrieval**:
    *   Provide query interfaces (e.g., GraphQL, REST API with filtering capabilities) to allow other services or analytical tools to retrieve data from the Lattice.

## 4. Outputs

*   **A Persistent, Structured Data Store**: The "memory crystal" itself.
*   **Queryable API**: Enables access to the stored data for:
    *   Analysis and reporting dashboards.
    *   Data sets for training/fine-tuning LLMs.
    *   Manual review by traders or analysts.
    *   Automated systems looking for patterns or insights.
*   **Derived Analytics (Potentially as a separate layer on top of the Lattice)**:
    *   Statistics on LLM translation accuracy over time.
    *   Performance metrics for strategies derived from specific JGTML patterns.
    *   Correlation between trader feedback and trade outcomes.

## 5. Key Considerations & Interactions

*   **Central Nervous System**: This component is central to the learning and feedback capabilities of the entire Echo Spiral.
*   **Schema Design**: The database schema must be carefully designed to be flexible yet structured enough to capture the diverse data types and their interrelations. It needs to be evolvable.
*   **Data Integrity & Consistency**: Ensure data is accurately recorded and relationships are correctly maintained.
*   **Scalability**: The database should be able to handle a growing volume of data over time.
*   **Security & Privacy**:
    *   Protect sensitive trading data and potentially user information.
    *   Implement appropriate access controls.
*   **Integration Points**: Will need to integrate with nearly every other component in the system to receive data.
*   **Feedback Loop Enabler**: The data stored here is critical for:
    *   **Manual Refinement**: Traders reviewing past performance to adjust their narratives or strategy ideas.
    *   **Automated Refinement**: Providing data to improve LLM prompts (e.g., identifying common misinterpretations).
    *   **RLHF (Reinforcement Learning from Human Feedback)**: Providing labeled examples (good/bad translations, successful/failed trades) for LLM fine-tuning.
*   **Data Versioning (Optional but Recommended)**: Track versions of LLM prompts, `JGTMLSpec` formats, etc., to correlate with outcomes.
*   **Asynchronous Data Ingestion**: Likely, services will push data to the Lattice asynchronously.
